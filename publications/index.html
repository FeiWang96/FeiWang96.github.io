<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Fei Wang </title> <meta name="author" content="Fei Wang"> <meta name="description" content="Fei Wang's personal website. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%8C&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://feiwang96.github.io/publications/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Fei</span> Wang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <p>See <a href="https://scholar.google.com/citations?user=N1O2KT8AAAAJ" rel="external nofollow noopener" target="_blank">Google Scholar</a> for an up-to-date list.</p> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mdpo-480.webp 480w,/assets/img/publication_preview/mdpo-800.webp 800w,/assets/img/publication_preview/mdpo-1400.webp 1400w," sizes="600px" type="image/webp"></source> <img src="/assets/img/publication_preview/mdpo.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mdpo.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024mdpo" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2406.11839" target="_blank" rel="external nofollow noopener"><b>mDPO: Conditional Preference Optimization for Multimodal Large Language Models</b></a></div> <div class="author"> <b>Fei Wang</b>, <span> Wenxuan Zhou </span> , <span> James Y Huang </span> , <span> Nan Xu </span> , <span> Sheng Zhang </span> , <span> Hoifung Poon </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Proceedings of <b>EMNLP</b></em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://feiwang96.github.io/mDPO/" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Website</span></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="wang2024data" class="col-sm-8"> <div class="title"><b>Data Advisor: Constitutional Data Curation for Safety Alignment of Large Language Models</b></div> <div class="author"> <b>Fei Wang</b>, <span> Ninareh Mehrabi </span> , <span> Palash Goyal </span> , <span> Rahul Gupta </span> , <span> Kai-Wei Chang </span> , and <span> Aram Galstyan </span> </div> <div class="periodical"> <em>In Proceedings of <b>EMNLP</b></em> , 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="du2024llms" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2406.16253" target="_blank" rel="external nofollow noopener"><b>Llms assist nlp researchers: Critique paper (meta-) reviewing</b></a></div> <div class="author"> <span> Jiangshu Du </span> , <span> Yibo Wang </span> , <span> Wenting Zhao </span> , <span> Zhongfen Deng </span> , <span> Shuaiqi Liu </span> , <span> Renze Lou </span> , <span> Henry Peng Zou </span> , <span> Pranav Narayanan Venkit </span> , <span> Nan Zhang </span> , <span> Mukund Srinath </span> , <span> Haoran Ranran Zhang </span> , <span> Vipul Gupta </span> , <span> Yinghui Li </span> , <span> Tao Li </span> , <b>Fei Wang</b>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? ' others' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of <b>EMNLP</b></em> , 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="liu2024monotonic" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2403.16038" target="_blank" rel="external nofollow noopener"><b>Monotonic Paraphrasing Improves Generalization of Language Model Prompting</b></a></div> <div class="author"> <span> Qin Liu </span> , <b>Fei Wang</b>, <span> Nan Xu </span> , <span> Tianyi Yan </span> , <span> Tao Meng </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Findings of <b>EMNLP</b></em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://github.com/luka-group/MonoPara" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="yan2024contrastive" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2402.11138" target="_blank" rel="external nofollow noopener"><b>Contrastive Instruction Tuning</b></a></div> <div class="author"> <span> Tianyi Yan </span> , <b>Fei Wang</b>, <span> James Y Huang </span> , <span> Wenxuan Zhou </span> , <span> Fan Yin </span> , <span> Aram Galstyan </span> , <span> Wenpeng Yin </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Findings of <b>ACL</b></em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://github.com/luka-group/CoIN" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="liu2023shortcuts" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2305.14910" target="_blank" rel="external nofollow noopener"><b>From Shortcuts to Triggers: Backdoor Defense with Denoised PoE</b></a></div> <div class="author"> <span> Qin Liu </span> , <b>Fei Wang</b>, <span> Chaowei Xiao </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Proceedings of <b>NAACL</b></em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://github.com/luka-group/DPoE" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="liu2023rethinking" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2312.16702" target="_blank" rel="external nofollow noopener"><b>Rethinking Tabular Data Understanding with Large Language Models</b></a></div> <div class="author"> <span> Tianyang Liu </span> , <b>Fei Wang</b>, and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Proceedings of <b>NAACL</b></em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://github.com/Leolty/tablellm" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="xu2024instructional" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2401.12255" target="_blank" rel="external nofollow noopener"><b>Instructional Fingerprinting of Large Language Models</b></a></div> <div class="author"> <span> Jiashu Xu </span> , <b>Fei Wang*</b>, <span> Mingyu Derek Ma* </span> , <span> Pang Wei Koh </span> , <span> Chaowei Xiao </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Proceedings of <b>NAACL</b></em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://github.com/cnut1648/Model-Fingerprint" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="li2023deceiving" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2311.09702" target="_blank" rel="external nofollow noopener"><b>Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?</b></a></div> <div class="author"> <span> Bangzheng Li </span> , <span> Ben Zhou </span> , <b>Fei Wang</b>, <span> Xingyu Fu </span> , <span> Dan Roth </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Proceedings of <b>NAACL</b></em> , 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="xu2023instructions" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2305.14710" target="_blank" rel="external nofollow noopener"><b>Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models</b></a></div> <div class="author"> <span> Jiashu Xu </span> , <span> Mingyu Derek Ma </span> , <b>Fei Wang</b>, <span> Chaowei Xiao </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Proceedings of <b>NAACL</b></em> , 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="xu2023cognitive" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2311.09827" target="_blank" rel="external nofollow noopener"><b>Cognitive overload: Jailbreaking large language models with overloaded logical thinking</b></a></div> <div class="author"> <span> Nan Xu </span> , <b>Fei Wang</b>, <span> Ben Zhou </span> , <span> Bang Zheng Li </span> , <span> Chaowei Xiao </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Findings of <b>NAACL</b></em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://github.com/luka-group/CognitiveOverload" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/muirbench-480.webp 480w,/assets/img/publication_preview/muirbench-800.webp 800w,/assets/img/publication_preview/muirbench-1400.webp 1400w," sizes="600px" type="image/webp"></source> <img src="/assets/img/publication_preview/muirbench.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="muirbench.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024muirbench" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2406.09411" target="_blank" rel="external nofollow noopener"><b>MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding</b></a></div> <div class="author"> <b>Fei Wang*</b>, <span> Xingyu Fu* </span> , <span> James Y Huang </span> , <span> Zekun Li </span> , <span> Qin Liu </span> , <span> Xiaogeng Liu </span> , <span> Mingyu Derek Ma </span> , <span> Nan Xu </span> , <span> Wenxuan Zhou </span> , <span> Kai Zhang </span> , and <span> others </span> </div> <div class="periodical"> <em>In arXiv preprint</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://muirbench.github.io/" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Website</span></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="wang2024instructions" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2403.06326" target="_blank" rel="external nofollow noopener"><b>From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification</b></a></div> <div class="author"> <b>Fei Wang</b>, <span> Chao Shang </span> , <span> Sarthak Jain </span> , <span> Shuai Wang </span> , <span> Qiang Ning </span> , <span> Bonan Min </span> , <span> Vittorio Castelli </span> , <span> Yassine Benajiba </span> , and <span> Dan Roth </span> </div> <div class="periodical"> <em>In arXiv preprint</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="xu2024introspection" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2407.00902" target="_blank" rel="external nofollow noopener"><b>From Introspection to Best Practices: Principled Analysis of Demonstrations in Multimodal In-Context Learning</b></a></div> <div class="author"> <span> Nan Xu </span> , <b>Fei Wang</b>, <span> Sheng Zhang </span> , <span> Hoifung Poon </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In arXiv preprint</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="yao2024privacy" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2402.08227" target="_blank" rel="external nofollow noopener"><b>Privacy-Preserving Language Model Inference with Instance Obfuscation</b></a></div> <div class="author"> <span> Yixiang Yao </span> , <span> \textbfFei \textbfWang </span> , <span> Srivatsan Ravi </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In arXiv preprint</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="huang2024offset" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2404.11045" target="_blank" rel="external nofollow noopener"><b>Offset Unlearning for Large Language Models</b></a></div> <div class="author"> <span> James Y Huang </span> , <span> Wenxuan Zhou </span> , <b>Fei Wang</b>, <span> Fred Morstatter </span> , <span> Sheng Zhang </span> , <span> Hoifung Poon </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In arXiv preprint</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://github.com/luka-group/Delta-Unlearning" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="li2024famicom" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2406.11243" target="_blank" rel="external nofollow noopener"><b>FamiCom: Further Demystifying Prompts for Language Models with Task-Agnostic Performance Estimation</b></a></div> <div class="author"> <span> Bangzheng Li </span> , <span> Ben Zhou </span> , <span> Xingyu Fu </span> , <span> \textbfFei \textbfWang </span> , <span> Dan Roth </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In arXiv preprint</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="chaves2024training" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2403.08002" target="_blank" rel="external nofollow noopener"><b>Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging</b></a></div> <div class="author"> <span> Juan Manuel Zambrano Chaves^* </span> , <span> Shih-Cheng Huang^* </span> , <span> Yanbo Xu^* </span> , <span> Hanwen Xu^* </span> , <span> Naoto Usuyama^* </span> , <span> Sheng Zhang^* </span> , <b>Fei Wang</b>, <span> Yujia Xie </span> , <span> Mahmoud Khademi </span> , <span> Ziyi Yang </span> , <span> Hany Awadalla </span> , <span> Julia Gong </span> , <span> Houdong Hu </span> , <span> Jianwei Yang </span> , <span> Chunyuan Li </span> , and <span class="more-authors" title="click to view 11 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '11 more authors' ? 'Jianfeng Gao, Yu Gu, Cliff Wong, Mu Wei, Tristan Naumann, Muhao Chen, Matthew P. Lungren, Serena Yeung-Levy, Curtis P. Langlotz, Sheng Wang, Hoifung Poon' : '11 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">11 more authors</span> </div> <div class="periodical"> <em>In arXiv preprint</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="wang2023causal" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2305.14695" target="_blank" rel="external nofollow noopener"><b>A Causal View of Entity Bias in (Large) Language Models</b></a></div> <div class="author"> <b>Fei Wang</b>, <span> Wenjie Mo </span> , <span> Yiwei Wang </span> , <span> Wenxuan Zhou </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Findings of <b>EMNLP</b></em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://github.com/luka-group/Causal-View-of-Entity-Bias" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="nan2023dense" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2310.18619" target="_blank" rel="external nofollow noopener"><b>Dense Retrieval as Indirect Supervision for Large-space Decision Making</b></a></div> <div class="author"> <span> Nan Xu </span> , <b>Fei Wang</b>, <span> Mingtao Dong </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Findings of <b>EMNLP</b></em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://github.com/luka-group/DDR" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="wang2023entred" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2305.13551" target="_blank" rel="external nofollow noopener"><b>How Fragile is Relation Extraction under Entity Replacements?</b></a></div> <div class="author"> <span> Yiwei Wang </span> , <span> Bryan Hooi </span> , <b>Fei Wang</b>, <span> Yujun Cai </span> , <span> Yuxuan Liang </span> , <span> Wenxuan Zhou </span> , <span> Jing Tang </span> , <span> Manjuan Duan </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Proceedings of <b>CoNLL</b></em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://github.com/wangywUST/ENTRED" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="wang2023self" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2309.10891" target="_blank" rel="external nofollow noopener"><b>Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer</b></a></div> <div class="author"> <b>Fei Wang</b>, <span> Kuan-Hao Huang </span> , <span> Kai-Wei Chang </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Proceedings of <b>AACL</b></em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/luka-group/SALT" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>Zero-shot cross-lingual transfer is a central task in multilingual NLP, allowing models trained in languages with more sufficient training resources to generalize to other low-resource languages. Earlier efforts on this task use parallel corpora, bilingual dictionaries, or other annotated alignment data to improve cross-lingual transferability, which are typically expensive to obtain. In this paper, we propose a simple yet effective method, SALT, to improve the zero-shot cross-lingual transfer of the multilingual pretrained language models without the help of such external data. By incorporating code-switching and embedding mixup with self-augmentation, SALT effectively distills cross-lingual knowledge from the multilingual PLM and enhances its transferability on downstream tasks. Experimental results on XNLI and PAWS-X show that our method is able to improve zero-shot cross-lingual transferability without external data. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="wang2023robust" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2305.17627" target="_blank" rel="external nofollow noopener"><b>Robust Natural Language Understanding with Residual Attention Debiasing</b></a></div> <div class="author"> <b>Fei Wang*</b>, <span> James Y. Huang* </span> , <span> Tianyi Yan </span> , <span> Wenxuan Zhou </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Findings of <b>ACL</b></em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/luka-group/READ" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>Natural language understanding (NLU) models often suffer from unintended dataset biases. Among bias mitigation methods, ensemble-based debiasing methods, especially product-of-experts (PoE), have stood out for their impressive empirical success. However, previous ensemble-based debiasing methods typically apply debiasing on top-level logits without directly addressing biased attention patterns. Attention serves as the main media of feature interaction and aggregation in PLMs and plays a crucial role in providing robust prediction. In this paper, we propose REsidual Attention Debiasing (READ), an end-to-end debiasing method that mitigates unintended biases from attention. Experiments on three NLU tasks show that READ significantly improves the performance of BERT-based models on OOD data with shortcuts removed, including +12.9% accuracy on HANS, +11.0% accuracy on FEVER-Symmetric, and +2.7% F1 on PAWS. Detailed analyses demonstrate the crucial role of unbiased attention in robust NLU models and that READ effectively mitigates biases in attention.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="dixit2023improving" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2305.14981" target="_blank" rel="external nofollow noopener"><b>Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality</b></a></div> <div class="author"> <span> Tanay Dixit </span> , <b>Fei Wang</b>, and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Proceedings of <b>ACL</b></em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/tanay2001/EFactSum" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose EFACTSUM (i.e., Effective Factual Summarization), a candidate summary generation and ranking technique to improve summary factuality without sacrificing summary quality. We show that using a contrastive learning framework with our refined candidate summaries leads to significant gains on both factuality and similarity-based metrics. Specifically, we propose a ranking strategy in which we effectively combine two metrics, thereby preventing any conflict during training. Models trained using our approach show up to 6 points of absolute improvement over the base model with respect to FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either similarity-based metrics or absractiveness.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="wang2022salience" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2210.12330" target="_blank" rel="external nofollow noopener"><b>Salience Allocation as Guidance for Abstractive Summarization</b></a></div> <div class="author"> <b>Fei Wang*</b>, <span> Kaiqiang Song* </span> , <span> Hongming Zhang </span> , <span> Lifeng Jin </span> , <span> Sangwoo Cho </span> , <span> Wenlin Yao </span> , <span> Xiaoyang Wang </span> , <span> Muhao Chen </span> , and <span> Dong Yu </span> </div> <div class="periodical"> <em>In Proceedings of <b>EMNLP</b></em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/tencent-ailab/season" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>Abstractive summarization models typically learn to capture the salient information from scratch implicitly. Recent literature adds extractive summaries as guidance for abstractive summarization models to provide hints of salient content and achieves better performance. However, extractive summaries as guidance could be over strict, leading to information loss or noisy signals. Furthermore, it cannot easily adapt to documents with various abstractiveness. As the number and allocation of salience content pieces vary, it is hard to find a fixed threshold deciding which content should be included in the guidance. In this paper, we propose a novel summarization approach with a flexible and reliable salience guidance, namely SEASON (SaliencE Allocation as Guidance for Abstractive SummarizatiON). SEASON utilizes the allocation of salience expectation to guide abstractive summarization and adapts well to articles in different abstractiveness. Automatic and human evaluations on two benchmark datasets show that the proposed method is effective and reliable. Empirical results on more than one million news articles demonstrate a natural fifteen-fifty salience split for news article sentences, providing a useful insight for composing news articles.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="xu2022does" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2205.12640" target="_blank" rel="external nofollow noopener"><b>Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing</b></a></div> <div class="author"> <span> Nan Xu </span> , <b>Fei Wang</b>, <span> Bangzheng Li </span> , <span> Mingtao Dong </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Proceedings of <b>EMNLP</b></em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/luka-group/DiagnoseET" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>The entity typing task aims at predicting one or more words or phrases that describe the type(s) of a specific mention in a sentence. Due to shortcuts from surface patterns to annotated entity labels and biased training, existing entity typing models are subject to the problem of spurious correlations. To comprehensively investigate the faithfulness and reliability of entity typing methods, we first systematically define distinct kinds of model biases that are reflected mainly from spurious correlations. Particularly, we identify six types of existing model biases, including mention-context bias, lexical overlapping bias, named entity bias, pronoun bias, dependency bias, and overgeneralization bias. To mitigate these model biases, we then introduce a counterfactual data augmentation method. By augmenting the original training set with their bias-free counterparts, models are forced to fully comprehend the sentences and discover the fundamental cues for entity typing, rather than relying on spurious correlations for shortcuts. Experimental results on the UFET dataset show that our counterfactual data augmentation approach helps improve generalization of different entity typing models with consistently better performance on both in- and out-of-distribution test sets.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="wang2022robust" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2205.03972" target="_blank" rel="external nofollow noopener"><b>Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning</b></a></div> <div class="author"> <b>Fei Wang</b>, <span> Zhewei Xu </span> , <span> Pedro Szekely </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Proceedings of <b>NAACL</b></em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/luka-group/Lattice" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>Controlled table-to-text generation seeks to generate natural language descriptions for highlighted subparts of a table. Previous SOTA systems still employ a sequence-to-sequence generation method, which merely captures the table as a linear structure and is brittle when table layouts change. We seek to go beyond this paradigm by (1) effectively expressing the relations of content pieces in the table, and (2) making our model robust to content-invariant structural transformations. Accordingly, we propose an equivariance learning framework, which encodes tables with a structure-aware self-attention mechanism. This prunes the full self-attention structure into an order-invariant graph attention that captures the connected graph structure of cells belonging to the same row or column, and it differentiates between relevant cells and irrelevant cells from the structural perspective. Our framework also modifies the positional encoding mechanism to preserve the relative position of tokens in the same cell but enforce position invariance among different cells. Our technology is free to be plugged into existing table-to-text generation models, and has improved T5-based models to offer better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo, we preserve promising performance, while previous SOTA systems, even with transformation-based data augmentation, have seen significant performance drops.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="wang2022zero" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2022.mmnlu-1.6/" target="_blank" rel="external nofollow noopener"><b>Zero-shot cross-lingual sequence tagging as Seq2Seq generation for joint intent classification and slot filling</b></a></div> <div class="author"> <b>Fei Wang</b>, <span> Kuan-Hao Huang </span> , <span> Anoop Kumar </span> , <span> Aram Galstyan </span> , <span> Greg Ver Steeg </span> , and <span> Kai-Wei Chang </span> </div> <div class="periodical"> <em>In Proceedings of the MMNLU Workshop at EMNLP</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> </div> <div class="abstract hidden"> <p>The joint intent classification and slot filling task seeks to detect the intent of an utterance and extract its semantic concepts. In the zero-shot cross-lingual setting, a model is trained on a source language and then transferred to other target languages through multi-lingual representations without additional training data. While prior studies show that pre-trained multilingual sequence-to-sequence (Seq2Seq) models can facilitate zero-shot transfer, there is little understanding on how to design the output template for the joint prediction tasks. In this paper, we examine three aspects of the output template – (1) label mapping, (2) task dependency, and (3) word order. Experiments on the MASSIVE dataset consisting of 51 languages show that our output template significantly improves the performance of pre-trained cross-lingual language models.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="wang2021table" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2109.04053" target="_blank" rel="external nofollow noopener"><b>Table-based Fact Verification With Salience-aware Learning</b></a></div> <div class="author"> <b>Fei Wang</b>, <span> Kexuan Sun </span> , <span> Jay Pujara </span> , <span> Pedro Szekely </span> , and <span> Muhao Chen </span> </div> <div class="periodical"> <em>In Findings of <b>EMNLP</b></em> , 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/luka-group/Salience-aware-Learning" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>Tables provide valuable knowledge that can be used to verify textual statements. While a number of works have considered table-based fact verification, direct alignments of tabular data with tokens in textual statements are rarely available. Moreover, training a generalized fact verification model requires abundant labeled training data. In this paper, we propose a novel system to address these problems. Inspired by counterfactual causality, our system identifies token-level salience in the statement with probing-based salience estimation. Salience estimation allows enhanced learning of fact verification from two perspectives. From one perspective, our system conducts masked salient token prediction to enhance the model for alignment and reasoning between the table and the statement. From the other perspective, our system applies salience-aware data augmentation to generate a more diverse set of training instances by replacing non-salient terms. Experimental results on TabFact show the effective improvement by the proposed salience-aware learning techniques, leading to the new SOTA performance on the benchmark.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="sun2021tabular" class="col-sm-8"> <div class="title"><a href="https://dl.acm.org/doi/10.1145/3459637.3482484" target="_blank" rel="external nofollow noopener"><b>Tabular Functional Block Detection with Embedding-based Agglomerative Cell Clustering</b></a></div> <div class="author"> <span> Kexuan Sun </span> , <b>Fei Wang</b>, <span> Muhao Chen </span> , and <span> Jay Pujara </span> </div> <div class="periodical"> <em>In Proceedings of <b>CIKM</b></em> , 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> </div> <div class="abstract hidden"> <p>Tables are a widely-used format for data curation. The diversity of domains, layouts, and content of tables makes knowledge extraction challenging. Understanding table layouts is an important step for automatically harvesting knowledge from tabular data. Since table cells are spatially organized into regions, correctly identifying such regions and inferring their functional roles, referred to as functional block detection, is a critical part of understanding table layouts. Earlier functional block detection approaches fail to leverage spatial relationships and higher-level structure, either depending on cell-level predictions or relying on data types as signals for identifying blocks. In this paper, we introduce a flexible functional block detection method by applying agglomerative clustering techniques which merge smaller blocks into larger blocks using two merging strategies. Our proposed method uses cell embeddings with a customized dissimilarity function which utilizes local and margin distances, as well as block coherence metrics to capture cell, block, and table scoped features. Given the diversity of tables in real-world corpora, we also introduce a sampling-based approach for automatically tuning distance thresholds for each table. Experimental results show that our method improves over the earlier state-of-the-art method in terms of several evaluation metrics.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="wang2021retrieving" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2105.01736" target="_blank" rel="external nofollow noopener"><b>Retrieving Complex Tables with Multi-Granular Graph Representation Learning</b></a></div> <div class="author"> <b>Fei Wang</b>, <span> Kexuan Sun </span> , <span> Muhao Chen </span> , <span> Jay Pujara </span> , and <span> Pedro Szekely </span> </div> <div class="periodical"> <em>In Proceedings of <b>SIGIR</b></em> , 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/FeiWang96/GTR" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>The task of natural language table retrieval (NLTR) seeks to retrieve semantically relevant tables based on natural language queries. Existing learning systems for this task often treat tables as plain text based on the assumption that tables are structured as dataframes. However, tables can have complex layouts which indicate diverse dependencies between subtable structures, such as nested headers. As a result, queries may refer to different spans of relevant content that is distributed across these structures. Moreover, such systems fail to generalize to novel scenarios beyond those seen in the training set. Prior methods are still distant from a generalizable solution to the NLTR problem, as they fall short in handling complex table layouts or queries over multiple granularities. To address these issues, we propose Graph-based Table Retrieval (GTR), a generalizable NLTR framework with multi-granular graph representation learning. In our framework, a table is first converted into a tabular graph, with cell nodes, row nodes and column nodes to capture content at different granularities. Then the tabular graph is input to a Graph Transformer model that can capture both table cell content and the layout structures. To enhance the robustness and generalizability of the model, we further incorporate a self-supervised pre-training task based on graph-context matching. Experimental results on two benchmarks show that our method leads to significant improvements over the current state-of-the-art systems. Further experiments demonstrate promising performance of our method on cross-dataset generalization, and enhanced capability of handling complex tables and fulfilling diverse query intents.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="wu2020corefqa" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/1911.01746" target="_blank" rel="external nofollow noopener"><b>CorefQA: Coreference resolution as query-based span prediction</b></a></div> <div class="author"> <span> Wei Wu </span> , <b>Fei Wang</b>, <span> Arianna Yuan </span> , <span> Fei Wu </span> , and <span> Jiwei Li </span> </div> <div class="periodical"> <em>In Proceedings of <b>ACL</b></em> , 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://github.com/ShannonAI/CorefQA" class="abstract badge" role="button" rel="external nofollow noopener" target="_blank"><span style="color:cadetblue">Code</span></a> </div> <div class="abstract hidden"> <p>In this paper, we present an accurate and extensible approach for the coreference resolution task. We formulate the problem as a span prediction task, like in machine reading comprehension (MRC): A query is generated for each candidate mention using its surrounding context, and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query. This formulation comes with the following key advantages: (1) The span prediction strategy provides the flexibility of retrieving mentions left out at the mention proposal stage; (2) In the MRC framework, encoding the mention and its context explicitly in a query makes it possible to have a deep and thorough examination of cues embedded in the context of coreferent mentions; and (3) A plethora of existing MRC datasets can be used for data augmentation to improve the model’s generalization capability. Experiments demonstrate significant performance boost over previous models, with 87.5 (+2.5) F1 score on the GAP benchmark and 83.1 (+3.5) F1 score on the CoNLL-2012 benchmark.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="meng2019glyce" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/1901.10125" target="_blank" rel="external nofollow noopener"><b>Glyce: Glyph-vectors for chinese character representations</b></a></div> <div class="author"> <span> Yuxian Meng* </span> , <span> Wei Wu* </span> , <b>Fei Wang*</b>, <span> Xiaoya Li* </span> , <span> Ping Nie </span> , <span> Fan Yin </span> , <span> Muyu Li </span> , <span> Qinghong Han </span> , <span> Xiaofei Sun </span> , and <span> Jiwei Li </span> </div> <div class="periodical"> <em>In Proceedings of <b>NeurIPS</b></em> , 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> </div> <div class="abstract hidden"> <p>It is intuitive that NLP tasks for logographic languages like Chinese should benefit from the use of the glyph information in those languages. However, due to the lack of rich pictographic evidence in glyphs and the weak generalization ability of standard computer vision models on character data, an effective way to utilize the glyph information remains to be found. In this paper, we address this gap by presenting Glyce, the glyph-vectors for Chinese character representations. We make three major innovations: (1) We use historical Chinese scripts (e.g., bronzeware script, seal script, traditional Chinese, etc) to enrich the pictographic evidence in characters; (2) We design CNN structures (called tianzege-CNN) tailored to Chinese character image processing; and (3) We use image-classification as an auxiliary task in a multi-task learning setup to increase the model’s ability to generalize. We show that glyph-based models are able to consistently outperform word/char ID-based models in a wide range of Chinese NLP tasks. We are able to set new state-of-the-art results for a variety of Chinese NLP tasks, including tagging (NER, CWS, POS), sentence pair classification, single sentence classification tasks, dependency parsing, and semantic role labeling. For example, the proposed model achieves an F1 score of 80.6 on the OntoNotes dataset of NER, +1.5 over BERT; it achieves an almost perfect accuracy of 99.8% on the Fudan corpus for text classification.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Fei Wang. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>